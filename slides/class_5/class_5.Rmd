---
title: "Análisis de Datos Categóricos"
subtitle: "Tablas de Contingencia"
author: "<br> Mauricio Bucca <br> [github.com/mebucca](https://github.com/mebucca) <br> mebucca@uc.cl"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["gentle-r.css","xaringan-themer.css"]
    df_print: default
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
editor_options: 
  
  chunk_output_type: console
---

class: inverse, center, middle
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(tidyverse)
library(xaringanthemer)
style_duo_accent(primary_color = "#C8102E", secondary_color = "#48A095",
                 background_color = "#f8f7f3",
                 header_font_google = google_font("Archivo"),
                 text_font_google   = google_font("Inconsolata"), 
                 link_color="#FDB913"

)
```

#Tablas de Contingencia


---
## Datos relaciones extra-matrimoniales, ejemplo

- Usaremos datos del artículo _A theory of extramarital affairs_ (Fair 1978), publicado en JPE. 

- Muestra de 601 individuos en USA. Información sobre relaciones extra-matrimoniales de cada individuo. También covariables como género, edad, años de matrimonio, paternidad, educación, etc.

--
```{r,  include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library("Ecdat")
library("tidyverse")
data(Fair)
affairsdata <- Fair %>% as_tibble()

# create a binary variable indicating whether person has ever had an affair
affairsdata <- affairsdata %>% 
	mutate(everaffair = case_when(nbaffairs == 0 ~ "Never", nbaffairs > 0 ~ "At least once") )

# display data as tibble
print(affairsdata)
```

---
## Datos relaciones extra-matrimoniales, ejemplo

<br>
--


Si estuviéramos interesados en estudiar la asociación entre género y haber tenido un "affair", el primer paso probablemente sería construir una tabla de este tipo:

--

```{r, include=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
ctable <- affairsdata %>% with(table(sex,everaffair))
print(ctable)
```


<br>
--
Este tipo de tablas se denomina _tablas de contingencia_.

---

## Tablas de contingencia

Una definición formal: una tabla de contingencia es una matriz que muestra la *distribución multivariada* de frecuencias de un número arbitrario de variables categóricas. 

<br>

Caso simple:

- $X$ y $Y$ son dos variables categóricas.

  - $X$ tiene $I$ categorías $\{i, \dots, I \}$ 

  - $Y$ tiene $J$ categorías $\{j, \dots, J \}$.


<br>
--

Una tabla rectangular que clasifica todas las combinaciones posibles de $X$ y $Y$ tendrá $I$ filas para las categorías de $X$, $J$ columnas para las categorías de $Y$, y $I \times J$ celdas.

---
## Tablas de contingencia


 - Una tabla que clasifica $n$ variables se denomina una tabla $n$-way
 
 - Una tabla con $I$ filas y $J$ columnas se denomina una $I \times J$ (léase I-por-J)

<br> 
--

Por ejemplo, la tabla en nuestro ejemplo es una tabla 2-way, 2-por-2
```{r, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
ctable <- affairsdata %>% with(table(sex,everaffair))
print(ctable)
```

<br>
--

- Una tabla de contingencia $n_{ij}$ (frecuencia conjunta) denota la frecuencia observada en la celda $i,j$, es decir, el número de casos presentes en la combinación $X=i \text{ y } Y=j$

- Usando la notación matricial $i$ indexa las filas y $j$ las columnas.

---
## Distribución conjunta y marginal de frecuencias


<br>
<br>
<br>

Estructura general de una tabla 2-way, $I \times J$

<br>

|           	| $Y=y_{1}$ 	| $Y=y_{2}$ 	| $\dots$ 	| $Y=y_{J}$ 	|   Total  	|
|:---------:	|:---------:	|:---------:	|:-------:	|:---------:	|:--------:	|
| $X=x_{1}$ 	|  $n_{11}$ 	|  $n_{12}$ 	| $\dots$ 	|  $n_{1J}$ 	| $n_{1+}$ 	|
| $X=x_{2}$ 	|  $n_{21}$ 	|  $n_{22}$ 	| $\dots$ 	|  $n_{2J}$ 	| $n_{2+}$ 	|
|  $\dots$  	|  $\dots$  	|  $\dots$  	| $\dots$ 	|  $\dots$  	|  $\dots$ 	|
| $X=x_{I}$ 	|  $n_{I1}$ 	|  $n_{I2}$ 	| $\dots$ 	|  $n_{IJ}$ 	| $n_{I+}$ 	|
|   Total   	|  $n_{+1}$ 	|  $n_{+2}$ 	| $\dots$ 	|  $n_{+J}$ 	| $n_{++}$ 	|


---
class: inverse, center, middle

# Estructura probabilística 


---
## Distribución conjunta

Supongamos que elegimos al azar un individuo de nuestra población. ¿Cual es la probabilidad de que pertenezca una celda dada de la tabla de contingencia?

<br>
--

Para cada frecuencia conjunta $n_{ij}$ en la tabla existe una probabilidad conjunta asociada $p_{ij}$, tal que

$$p_{ij} = \mathbb{P}(X = i, Y = j)$$


  - denota la probabilidad de que una observación muestreada al azar pertenezca a la celda $(i,j)$.

  - la colección de probabilidades $p_{ij}$ forma la .bold[distribución conjunta] de $X$ y $Y$, $f(x,y)$. 


--

### Estimación

Cuando trabajamos con muestras, esta probabilidad se puede estimar (MLE) a partir de las frecuencias en la tabla:

$$\hat{p}_{ij} = \frac{n_{ij}}{n}$$


---
## Distribución conjunta

En nuestro ejemplo,

```{r,  include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
# joint distibution
joint_dis <- ctable/sum(ctable); joint_dis  
```

<br>
--
Como con cualquier distribución de probabilidad, sabemos que los $p_{ij}$ suman a 1. 

--

Veamos en el caso de nuestro estimador:

Si $\hat{p}_{ij} = \frac{n_{ij}}{n}$, entonces 

$$\sum_{i} \sum_{j} \frac{n_{ij}}{n} = \frac{n}{n} = 1$$
---
## Distribuciones marginales

<br>

Podemos obtener la distribución marginal de las variables $X$ y $Y$ a partir de su distribución conjunta. 

<br>
--

- La distribución marginal de $X$ (filas) está dada por:    

$$p_{i+} = \sum_{j} p_{ij}$$
<br>
--

- La distribución marginal de $Y$ (columnas) está dada por:    

$$p_{+j} = \sum_{i} p_{ij}$$


---
## Distribuciones marginales

.pull-left[
Cuando trabajamos con una muestra podemos estimar las distribuciones marginales a partir de las proporciones muestrales. 
]
.pull-right[

```{r,  include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
# joint distibution
joint_dis <- ctable/sum(ctable); joint_dis  
```
]

--

```{r,  include=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
# marginal distribution rows
rowSums(joint_dis)
```

```{r,  include=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
# marginal distribution columns
colSums(joint_dis)
```

<br>
--
Como toda distribución de probabilidad, .bold[suma a 1].


---
## Distribución conjunta y marginal de probabilidades 

En resumen,

<br>

|           	| $Y=y_{1}$ 	| $Y=y_{2}$ 	| $\dots$ 	| $Y=y_{J}$ 	|   Total  	|
|:---------:	|:---------:	|:---------:	|:-------:	|:---------:	|:--------:	|
| $X=x_{1}$ 	|  $p_{11}$ 	|  $p_{12}$ 	| $\dots$ 	|  $p_{1J}$ 	| $p_{1+}$ 	|
| $X=x_{2}$ 	|  $p_{21}$ 	|  $p_{22}$ 	| $\dots$ 	|  $p_{2J}$ 	| $p_{2+}$ 	|
|  $\dots$  	|  $\dots$  	|  $\dots$  	| $\dots$ 	|  $\dots$  	|  $\dots$ 	|
| $X=x_{I}$ 	|  $p_{I1}$ 	|  $p_{I2}$ 	| $\dots$ 	|  $p_{IJ}$ 	| $p_{I+}$ 	|
|   Total   	|  $p_{+1}$ 	|  $p_{+2}$ 	| $\dots$ 	|  $p_{+J}$ 	| 1	|


<br>
Estructura general de probabilidades en una  tabla 2-way, $I \times J$


---
class: middle

## Distribucion conjunta y marginales

.center[
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(ggExtra)
library(gridExtra)

# Create all possible combinations of two discrete variables x and y
n_categories <- 5  # Number of categories for each variable, from 0 to 30
n_trials <- 3  # Number of trials for the binomial distribution
prob_x <- 0.6  # Success probability for x in the binomial distribution
prob_y_given_x <- 0.7  # Success probability for y given x in the binomial distribution

x <- 0:(n_categories - 1)
y <- 0:(n_categories - 1)

# Create a dataframe with all combinations of x and y
data_grid <- expand.grid(x = x, y = y)

# Simulate a 3D bell shape using the binomial distribution for both x and y
data_grid$z <- dbinom(data_grid$x, size = n_trials, prob = prob_x) * 
               dbinom(data_grid$y, size = n_trials, prob = prob_y_given_x)


# Create the main heatmap
p_main <- ggplot(data_grid, aes(x = x, y = y)) +
  geom_tile(aes(fill = z), color = "black", width = 1, height = 1) +
  scale_fill_gradient(low = "#FDB913", high = "#C8102E") +
  theme_minimal() +
  labs(
    title = "",
    x = "",
    y = "",
    fill = "Density"
  ) +
  theme(
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    legend.position = "none"
  )

# Create the marginal density plot for X and rotate it by 90 degrees
p_x <- ggplot(data_grid %>% group_by(x) %>% summarise(z = sum(z)), aes(x = x, y = z)) +
  geom_col(fill = "#FDB913") +
  coord_flip() +
  theme_void()  # Remove grid and axis labels

# Create the marginal density plot for Y
p_y <- ggplot(data_grid %>% group_by(y) %>% summarise(z = sum(z)), aes(x = y, y = z)) +
  geom_col(fill = "#FDB913") +
  theme_void()  # Remove grid and axis labels

# Combine all plots
grid.arrange(p_y, NULL, p_main, p_x, ncol = 2, nrow = 2, widths = c(4, 1), heights = c(1, 4))

```
]


---
## Distribuciones condicionales 

<br>

- Recuerden que $\mathbb{P}(Y=y \mid X=x)$ es la probabilidad de que la variable $Y$ tome valor $y$ si $X$ toma valor $x$.


--

- La distribución condicional $f(y \mid x)$ es una función que expresa la probabilidad que $Y$ tome cada uno de sus posibles valores $y$'s para $X$ fijo en cada uno de sus valores posibles $x$'s.

<br>
--

Por tanto,

-  En una tabla de contingencia podemos construir las distribuciones condicionales de las variables $X$ (o $Y$) fijando la otra variable en sus diferentes niveles.

--

- Normalmente nos referimos como la "variable independiente" a la variable que usamos para condicionar, mientras que la otra variable actúa como "variable dependiente". 

---
## Distribuciones condicionales 

<br>

.pull-left[
En nuestro ejemplo podemos construir la distribución condicional  de la variable `everaffair` dado `sex` usando la fórmula general para probabilidades condicionales:
]

.pull-right[
```{r, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
ctable <- affairsdata %>% with(table(sex,everaffair))
print(ctable)
```
]

<br>
<br>
--
\begin{align}
\mathbb{P}( \text{affair}=j | \text{ gender}=i  ) &= \frac{\mathbb{P}(\text{affair}=j , \text{ gender}=i )}{\mathbb{P}(\text{ gender}=i)} 
\end{align}

---
## Distribuciones condicionales 

<br>

.pull-left[
Sustituyendo las probabilidades  de la ecuación por sus respectivos estimadores podemos estimar las distribuciones condicionales en la tabla:
]
.pull-right[
```{r, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
ctable <- affairsdata %>% with(table(sex,everaffair))
print(ctable)
```
]

<br>
<br>
--

\begin{align}
 \hat{p}_{j | i} &= \frac{P(\text{affair}=j , \text{ gender}=i )}{P(\text{ gender}=i)} \\ \\
  &= \frac{\frac{n_{ij}}{n}}{\frac{\sum_{j} n_{ij}}{n}} = \frac{n_{ij}}{\sum_{j}n_{ij}} 
\end{align}

---

## Distribuciones condicionales 

.pull-left[
Sustituyendo las probabilidades  de la ecuación por sus respectivos estimadores podemos estimar las distribuciones condicionales en la tabla:
]
.pull-right[
```{r, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
ctable <- affairsdata %>% with(table(sex,everaffair))
print(ctable)
```
]

<br>

Por ejemplo, la probabilidad condicional de haber tenido un "affair" dado que el genero es mujer se estima de la siguiente manera:

\begin{align}
 \hat{p}_{ \text{had affair} | \text{women}} & = \frac{n_{11}}{\sum_{j}n_{1j}} \\ \\
 &= \frac{72}{72 + 243} = 0.23
\end{align}

---
## Distribuciones condicionales 

.pull-left[
Sustituyendo las probabilidades  de la ecuación por sus respectivos estimadores podemos estimar las distribuciones condicionales en la tabla:
]
.pull-right[
```{r, include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
ctable <- affairsdata %>% with(table(sex,everaffair))
print(ctable)
```
]

<br>

En términos más generales, la _distribución condicional_ de la variable `everaffair`, dado `sex` se estima del siguiente modo:

```{r}
ctable <- affairsdata %>% with(table(sex,everaffair)) # contingency table
ctable/ rowSums(ctable) # joint distribution /  marginal distribution gender 
#can also be computed using prop.table(ctable,1)
```

---
## Independencia estadística


- Recuerden, dos variables $X$ y $Y$ son independientes si al saber algo sobre $X$ no aprendemos nada sobre $Y$, y viceversa: $\mathbb{P}(Y|X) = \mathbb{P}(Y)$.


- Check:  $X \bot Y \iff \mathbb{P}(X,Y) = \mathbb{P}(X)\mathbb{P}(Y)$

<br>
--

.bold[Ejercicio rápido]:
Supongamos que el 60% de la población son mujeres, y el 50% ha tenido un "affair" al menos una vez. Si la probabilidad de tener un "affair" fuera independiente del género, ¿cuál sería la probabilidad de que, al seleccionar una persona al azar, encontremos una mujer que ha tenido un "affair"? 

<br>

.full-width.content-box-secondary[
.bolder[Respuesta]:
  $$\mathbb{P}(\text{affair},\text{mujer}) = \mathbb{P}(\text{affair})\mathbb{P}(\text{mujer}) = 0.6 \times 0.5 = 0.3$$
]


---
## Independencia estadística

Podemos usar esta propiedad para comprobar independencia en una tabla de contingencia.

<br>
--

- Si $X \bot Y$ las probabilidades conjuntas .bold[esperadas bajo el supuesto de independencia]  están dadas por:

$$\tilde{p}_{ij} = p_{i+} \times  p_{+j}$$
<br>
--

- Asimismo, las frecuencias esperadas bajo independencia están dadas por:

$$\tilde{n}_{ij} = n \times p_{i+} \times  p_{+j}$$

<br>
--

.bold[Importante]: noten que sólo necesitamos saber la distribución marginal de las variables para calcular las probabilidades y frecuencias esperadas bajo independencia. 

---
## Independencia estadística

.pull-left[
.bold[Distribución conjunta observada]

```{r, echo=FALSE}
# joint_dis <- ctable/sum(ctable); joint_dis  
joint_gender_affair <- ctable/sum(ctable); print(joint_gender_affair)
```

]
.pull-right[
.bold[Distribuciones marginales]
```{r, echo=FALSE}
# marginal gender 
margin_gender <- apply(joint_gender_affair,1,sum); print(margin_gender)
# marginal affair 
margin_affair <- apply(joint_gender_affair,2,sum); print(margin_affair)
```
]

<br>
--

.bold[Distribución conjunta esperada bajo independencia]

```{r}
# expected joint probs under independence 
joint_gender_affair_indep <- margin_gender %*% t(margin_affair)
print(joint_gender_affair_indep)
```

---
class: inverse, center, middle

## Test $\chi^{2}$ de indepencia estadística  

---
### Test $\chi^{2}$ de indepencia estadística 

Primer paso, testear que exista _algo_ de asociación: ¿son estas tablas _suficientemente distintas_? 

```{r, echo=FALSE}
# joint
joint_gender_affair <- ctable/sum(ctable); 
# marginal gender 
margin_gender <- apply(joint_gender_affair,1,sum)
# marginal affair 
margin_affair <- apply(joint_gender_affair,2,sum)
```

.pull-left[
.bold[Frecuencias observadas]
```{r, echo=FALSE}
# joint_dis <- ctable/sum(ctable); joint_dis  
print(ctable)
```
]

--

.pull-right[
.bold[Frecuencias esperadas bajo independencia]
```{r, echo=FALSE}
# expected joint probs under independence 
joint_gender_affair_indep <- margin_gender %*% t(margin_affair)
rownames(joint_gender_affair_indep) <- c("female","male")
ctable_independence <- sum(ctable)*joint_gender_affair_indep 
print(ctable_independence)
```
]

Donde cada frecuencia esperada bajo independencia está dada por: $\tilde{n}_{ij} = n \times \hat{p}_{i+} \times  \hat{p}_{+j}$

--

- El test Pearson $\chi^{2}$ ( $t$ ) mide el grado asociación en la tabla de la siguiente manera:

.content-box-secondary[
$$t =\sum_{\text{all k: } i,j} \frac{(n_{ij} - \tilde{n}_{ij})^{2}}{\tilde{n}_{ij}}$$
]

Un valor alto de  $t$ sugiere que las variables no son independientes.
--
Pero, ¿cuánto es "alto"?

---
### Test $\chi^{2}$ de indepencia estadística 

.bold[Nota:]
- Si $Z_{1}, \dots , Z_{k}$ son variables independientes y cada $Z \sim \text{Normal}(0,1)$, 
- Entonces la variable $Y = \sum_{k} Z^{2} \sim \chi^{2}_{k}$. $Y$ distribuye $\chi^{2}$ con $k$ grados de libertad.

<br>
--

.bold[Heuristica:]

- $t =\sum_{\text{all k: } i,j} \frac{(n_{ij} - \tilde{n}_{ij})^{2}}{\tilde{n}_{ij}}$

- Si no hay asociación entre las variables ( $H_{0}$ es verdadera ) entonces:  $t =\sum_{\text{all k: } i,j} \frac{(\text{algo cercano a cero})^{2}}{\tilde{n}_{ij}}$
--

.content-box-secondary[
Pearson demostró que si $H_{0}$ es veradera, entonces:
$$t \sim \chi_{df}^{2}, \quad  \text{ donde } \quad  df= (I-1)(J-1)$$

]

---
### Test $\chi^{2}$ de indepencia estadística 

.bold[p-value]: 

.content-box-secondary[
$$\mathbb{P}(t  > \hat{t} \mid H_{0})$$
]

equivalente a:

--

.content-box-secondary[
$$\mathbb{P}(\chi_{df}^{2}  > \hat{t})$$
]


```{css, echo=FALSE}
.pull-right ~ * { clear: unset; }
.pull-right + * { clear: both; }
```


---
### Test $\chi^{2}$ de indepencia estadística 

.pull-left[
.bold[Frecuencias observadas]
```{r, echo=FALSE}
ctable %>% print()
```
]

.pull-right[
.bold[Frecuencias esperadas bajo independencia]
```{r, echo=FALSE}
ctable_independence %>% print()
```
]

<br>
--

.bold[(O-E)^2/E]

```{r, warning=F, message=F}
(((ctable - ctable_independence)^(2))/ctable_independence) %>% print()
```


--

.bold[Test Chi-2 : ∑ (O-E)^2/E]

```{r, echo=FALSE}
our_chi2 <- (((ctable - ctable_independence)^(2))/ctable_independence) %>% sum(); print(our_chi2)
```

---
### Test $\chi^{2}$ de indepencia estadística 


.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8.5}
library("tidyverse")

df <- 1  # Degrees of freedom
our_chi2 <- 1.56  # Replace this with your value if different

mydata <- tibble(x = seq(from = 0, to = 10, by = 0.01), chi2 = dchisq(x, df))

plot <- ggplot(data = mydata, mapping = aes(x = x)) +
  geom_path(aes(y = chi2), size = 1.5, alpha = 0.8, color = "#C8102E") +
  geom_area(
    data = filter(mydata, x >= our_chi2),
    aes(y = chi2),
    fill = "#C8102E",
    alpha = 0.4
  ) +
  labs(
    y = "f(y)",
    x = "y",
    title = "Probability function Chi^2 con df=1"
  ) +
  geom_vline(xintercept = our_chi2, color = "#48A095", size = 1.5) +
  annotate(
    geom = "text",
    x = our_chi2 + 1.5,
    y = 0.3,
    label = expression(paste(hat(t), " = 1.56")),
    color = "black",
    parse = TRUE,
    size = 6
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 22),
    axis.text.x = element_text(size = 22),
    axis.title.y = element_text(size = 24),
    axis.title.x = element_text(size = 24),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

print(plot)

```
.bold[Para ser claros:] Si la hipótesis de independencia ( $H_{0}$ ) es cierta, nuestro test $t$ distribuye $\chi^{2}$ con  $df= (I-1)(J-1)=1$
]

--

.pull-right[
.bold[p-value]

$$\mathbb{P}(\chi_{df=1}^{2} \geq \hat{t} )$$
```{r, warning=F, message=F}
1- pchisq(our_chi2,df=1)
```

]


---
### Test $\chi^{2}$ de indepencia estadística 

.pull-left[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.width=8.5}
library("tidyverse")

df <- 1  # Degrees of freedom
our_chi2 <- 1.56  # Replace this with your value if different

mydata <- tibble(x = seq(from = 0, to = 10, by = 0.01), chi2 = dchisq(x, df))

plot <- ggplot(data = mydata, mapping = aes(x = x)) +
  geom_path(aes(y = chi2), size = 1.5, alpha = 0.8, color = "#C8102E") +
  geom_area(
    data = filter(mydata, x >= our_chi2),
    aes(y = chi2),
    fill = "#C8102E",
    alpha = 0.4
  ) +
  labs(
    y = "f(y)",
    x = "y",
    title = "Probability function Chi^2 con df=1"
  ) +
  geom_vline(xintercept = our_chi2, color = "#48A095", size = 1.5) +
  annotate(
    geom = "text",
    x = our_chi2 + 1.5,
    y = 0.3,
    label = expression(paste(hat(t), " = 1.56")),
    color = "black",
    parse = TRUE,
    size = 6
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 22),
    axis.text.x = element_text(size = 22),
    axis.title.y = element_text(size = 24),
    axis.title.x = element_text(size = 24),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

print(plot)
```
.bold[Para ser claros:] Si la hipótesis de independencia ( $H_{0}$ ) es cierta, nuestro $\text{test } \chi^{2}$ distribuye $\chi^{2}$ con  $df= (I-1)(J-1)=1$
]


.pull-right[
.bold[p-value]

$$\mathbb{P}(\chi_{df=1}^{2} \geq \hat{t})$$
```{r, warning=F, message=F}
1- pchisq(our_chi2,df=1)
```


```{r, warning=F, message=F}
# Versión automática
chisq.test(ctable,correct = FALSE)
```

]

--

.bold[Conclusión]: el valor obtenido en nuestro test no es un valor demasiado improbable bajo independencia. No tenemos evidencia fuerte para sostener que ambas variables están asociadas. 

---
class: inverse, center, middle

## Medidas de Asociación

---
##  Asociación en tablas de contingencia 

Las variables de una tabla de contingencia están asociadas si la distribución condicional de las variables es distinta de su distribución marginal. Formalmente, 

<br>

- $f_{Y \mid X}(Y \mid X) \neq f_{Y}(Y)$

y por tanto,

- $f_{X \mid Y}(X \mid Y) \neq f_{X}(X)$


---
##  Asociación en tablas de contingencia 

```{r,  include=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
library("tidyverse")
library("Ecdat")
library("cowplot")
theme_set(theme_cowplot())

data(Fair)
affairsdata <- Fair %>% as_tibble()

# create a binary variable indicating wether persons has ever had an affair
affairsdata <- affairsdata %>% 
	mutate(everaffair = case_when(nbaffairs == 0 ~ "Never", nbaffairs > 0 ~ "At least once") )

ctable <- affairsdata %>% with(table(sex,everaffair))
```

Continuando con nuestro ejemplo,

.pull-left[
$f(\text{everaffair} \mid \text{sex})$
```{r, warning=F, message=F}
prop.table(ctable,1)
```


$f(\text{everaffair})$
```{r, warning=F, message=F}
prop.table(apply(ctable,2,sum))
```

]

--

.pull-right[
Al parece que los hombres tienen una mayor probabilidad que las mujeres de haber tenido un "affair".

En lo que sigue vamos a usar este ejemplo para estudiar:

- Diferentes formas de cuantificar la asociación (o la ausencia de la misma) entre variables de una tabla de contingencia

- Evaluar si las diferencias observadas son o no más sustanciales de lo se esperaría debido al mero azar.
]


---
class: inverse, center, middle

## Medidas de Asociación
### Diferencia de proporciones

---
## Diferencia de proporciones

- Supongamos que tenemos una tabla de contingencia 2-ways que cruza las variables binarias $X$ (independiente) y $Y$ (dependiente).  Éxito se codifica con valor 1 y el fracaso con el valor 0.

- Para detectar la asociación necesitamos medir diferencias en la distribución de $Y$ condicional en $X$

--
La diferencia de proporciones cuantifica estas diferencias de la siguiente manera:

$$\delta = \mathbb{P}(Y=1 \mid X=1) - \mathbb{P}(Y=1 \mid X=0)$$
--

Noten que $\delta \in [-1,1]$ donde $\delta=0$ indica proporciones iguales. 

--

Volviendo a nuestro ejemplo, $\hat{p}_{H}$, llamemos y a la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ a la proporción de mujeres que han tenido una aventura. La diferencia de proporciones se define simplemente como:

\begin{align}
  \hat{\delta} &= \hat{p}_{H} - \hat{p}_{M} \\ \\
  &= 0.273 - 0.229 \\ \\
  &= 0.044
\end{align}

---
## Diferencia de proporciones

Dos consideraciones importantes:

1) La diferencia de proporciones debe estar adecuadamente definida en términos de una variable dependiente y otra independiente. La razón es que, en general:

$$\mathbb{P}(Y=1 \mid X=1) - \mathbb{P}(Y=1 \mid X=0) \neq  \mathbb{P}(X=1 \mid Y=1) - \mathbb{P}(X=1 \mid Y=0)$$

--

En nuestro ejemplo:

```{r, warning=F, message=F}
prop.table(ctable,2)
```

Si tratamos género como variable dependiente y definimos "mujeres" como la categoría de éxito, la diferencia en las proporciones es $\delta = 0.48 - 0.54 = -0.06$. 

---
## Diferencia de proporciones

2) La diferencia de proporciones es una estadística intuitiva y fácil de interpretar, pero por sí sola puede ser engañosa cuando las proporciones son ambas cercanas a cero. Consideremos los dos casos hipotéticos siguientes:

\begin{align}
  \text{Caso 1: } p_{1a}=0.410 \text{ and } p_{1b}=0.401  \\ \\
  \text{aquí: } \delta_{1} = 0.009
\end{align}

--
y

\begin{align}
  \text{Caso 2: } p_{2a}=0.010 \text{ and } p_{2b}=0.001  \\ \\
  \text{aquí: } \delta_{2} = 0.009
\end{align}

--
¿Problemas? En el caso 1 ambas porciones son, según todos los indicios, casi idénticas. En el caso 2, sin embargo, ambas proporciones son similares en términos absolutos, por muy diferentes en términos relativos: $0.010$ es 10 veces mayor $0.001$.


---
class: inverse, center, middle

## Medidas de Asociación
### Odds Ratio


---
## Odds Ratio

--
<br>

- Odds ratio ( $\theta$ ) es una medida fundamental de asociación. 

--

- Parámetro de interés en el modelo más importante de datos categóricos: regresión logística.

--

- $\theta$ está formulada para tablas de 2-por-2, pero también puede calcularse para tablas de mayor dimensión:

  - Toda tabla $n$-ways, $I \times J$, puede ser re-escrita como  $(I-1) \times (J-1) \times (n-1)$ tablas de 2-por-2.

---
### Odds

La Odds Ratio es el ratio de dos "odds", donde las "odds" una variable binaria $Y$ se definen como: 

<br>

\begin{align}
  \text{odds} &= \frac{\mathbb{P}(Y=1)}{1-\mathbb{P}(Y=1)} \\ \\
              &=  \frac{p}{1-p}
\end{align}

<br>
--

Por ejemplo, si $Y$ tiene una probabilidad de éxito $p=0.75$, las odds de éxito son $\text{odds}=\frac{0.75}{0.25} = 3$

- Esto significa que las "chances" de éxito son 3:1


---
### Odds

<br>

las Odds son funciones de probabilidades y, por lo tanto, las probabilidades también pueden expresarse en función de las odds. Formalmente:

<br>

$$p = \frac{\text{odds}}{1 + \text{odds}}$$
<br>
--

Siguiendo con ejemplo anterior, si sabemos que las odds de éxito son igual a 3, entonces la probabilidad $p$ de éxito es:

<br>
\begin{align}
p &= \frac{3}{1 + 3} \\ \\
  &= 0.75
\end{align}



---
## Odds Ratio

Las .bold[odds] resumen la distribución de una sola variable binaria. Para medir la asociación entre dos de estas variables en una tabla podemos calcular la .bold[odds *ratio*]. 

--

Si $X$ e $Y$ son las variables binarias -- independiente y dependiente -- la distribución condicional $f(Y \mid X)$ se puede resumir con dos .bold[odds]:

\begin{align}
  \text{odds}_{0} &=  \frac{\mathbb{P}(Y=1 | X=0) }{1 - \mathbb{P}(Y=1 | X=0) } \quad \text{y} \\ \\
  \text{odds}_{1} &=  \frac{\mathbb{P}(Y=1 | X=1) }{1 - \mathbb{P}(Y=1 | X=1) } 
\end{align}

--

La .bold[odds *ratio*], por tanto, es:

\begin{align}
  \theta &= \frac{\text{odds}_{1}}{\text{odds}_{0}} \\ \\\
\end{align}


---
## Odds Ratio

Volviendo a nuestro ejemplo,

```{r,echo=FALSE}
prop.table(ctable,1)
```

Si $\hat{p}_{H}$ es la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ es la proporción de mujeres que han tenido una aventura. 


\begin{align}
  \hat{\theta} = \frac{\text{odds}_{H}}{\text{odds}_{M}} &= \\
         &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
         &= \frac{0.273/0.727}{0.229/0.771} \\ \\
         &= \frac{0.38}{0.30} = 1.27
\end{align}

---
## Odds Ratio

Dado que estas proporciones se *estiman* a partir de los recuentos de la tabla, $\hat{\theta}$ también puede expresarse de la siguiente manera, denominada .bold[cross-product ratio].

En nuestro ejemplo,

.pull-left[
```{r, echo=FALSE}
ctable
```
]

.pull-right[
\begin{align}
  \hat{\theta} &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
   &= \frac{\frac{n_{21}}{n_{2+}} / \frac{n_{22}}{n_{2+}}}{\frac{n_{11}}{n_{1+}}/ \frac{n_{12}}{n_{1+} }} = \frac{n_{21}/n_{22}}{n_{11}/n_{12}}  \\ \\
   &= \frac{n_{21} \times n_{12}}{n_{22} \times n_{11}} \\ \\
\end{align}
]

--

```{r, warning=F, message=F}
Theta = (ctable[2,1]*ctable[1,2])/(ctable[2,2]*ctable[1,1]); Theta
```

---
## Odds Ratio

```{r, warning=F, message=F}
Theta = (ctable[2,1]*ctable[1,2])/(ctable[2,2]*ctable[1,1]); Theta
```

.bold[Interpretación]: las odds de que un hombre tenga affair son 1,27 veces mayores que las de una mujer, es decir, 27% más altas. 

Notice that:

- $\theta \in [0,\infty+]$

--

- $\theta=1$ indica igualdad de odds y, por lo tanto, independencia

--

- $\theta > 1$ indica que el éxito es más probable para el grupo en el numerador (hombres en este caso)

--

- $\theta < 1$ indica que el éxito es más probable para el grupo en el denominador (mujeres en este caso)

--

- Valores lejos de 1, en cualquier dirección, representan una fuerte evidencia contra independencia

---
### Propiedades de la Odds Ratio

1) Invirtiendo el orden de las filas o columnas obtenemos el inverso la odds ratio original.

```{r,echo=FALSE}
prop.table(ctable,1)
```

Si $\hat{p}_{H}$ es la proporción de hombres que han tenido una aventura y $\hat{p}_{M}$ es la proporción de mujeres que han tenido una aventura. 

.pull-left[
\begin{align}
  \hat{\theta}_{HM} &= \frac{\hat{p}_{H}/(1 - \hat{p}_{H})}{\hat{p}_{M}/(1 - \hat{p}_{M})} \\ \\
         &= \frac{0.38}{0.30} \\ \\
         &= 1.27
\end{align}
]

.pull-right[
\begin{align}
  \hat{\theta}_{HM} &= \frac{\hat{p}_{M}/(1 - \hat{p}_{M})}{\hat{p}_{H}/(1 - \hat{p}_{H})} \\ \\
         &= \frac{0.30}{0.38} \\ \\
         &= 1/1.27 = 0.79
\end{align}
]

.full-width[
Tanto $\theta$ como $1/\theta$ expresan el .bold[mismo grado de asociación].
]

---
### Propiedades de la Odds Ratio

2) A diferencia de las otras medidas, la odds ratio no varia en función de que  variable actúa como dependiente e independiente. En otras palabras, no es necesario identificar una variable independiente para estimar correctamente $\theta$

--

En nuestro ejemplo, tomando género como variable dependiente, donde $\hat{p}_{A}$ es la probabilidad de ser hombre entre personar que han tenido un affair y $\hat{p}_{NA}$ es la misma probabilidad para personas que nunca han tenido un affair, la odd-ratio de ser hombre es:

.pull-left[
```{r, echo=FALSE}
prop.table(ctable,2)
```
]

.pull-right[
\begin{align}
  \hat{\theta} &= \frac{\hat{p}_{A}/(1 - \hat{p}_{A})}{\hat{p}_{NA}/(1 - \hat{p}_{NA})} \\ \\
         &= \frac{0.52/0.48}{0.46/0.54} \\ \\
         &= \frac{1.1}{0.85} \\ \\
         &= 1.27
\end{align}
]

---
### Propiedades de la Odds Ratio

3) La Odds Ratio es .bold[margins-free]: la odds ratio de una tabla de contingencia no se ven alteradas por el "escalamiento" (multiplicación por una constante) de filas o columnas.  

--

.pull-top[
.pull-left[
.bold[Movilidad educacional 1980]
```{r, echo=FALSE}
pais_1980 <- matrix(c(160,20,20,20),2,2)
colnames(pais_1980) <- c("Hijos: NU","Hijos: U")
rownames(pais_1980) <- c("Padre: NU","Padres: U")
knitr::kable(pais_1980)
```
]
.pull-right[
.bold[Movilidad educacional 2020]
```{r, echo=FALSE}
pais_2020 <- matrix(c(160,20,80,80),2,2)
colnames(pais_2020) <- c("Hijos: NU","Hijos: U")
rownames(pais_2020) <-  c("Padre: NU","Padres: U")
knitr::kable(pais_2020)
```
]
]

--

<br>

.pull-bottom[
.pull-left[
- El .bold[13%] de los hijos padres sin estudios universitarios obtenía un grado universitario
]
.pull-right[
- El .bold[33%] de los hijos padres sin estudios universitarios obtiene un grado universitario
]
]

<br>
--

- .bold[Diferencia de proporciones]: 0.33 - 0.13 = 0.2

--

.full-width[
.bold[Radio Duna:] _"En Chile practicamente se ha triplicado la movilidad educacional en los últimos 30 años, y aún así esta gente reclama..."_
]

--
.bold[Correcto?]


```{css, echo=FALSE}
.pull-right ~ * { clear: unset; }
.pull-right + * { clear: both; }
```
---
### Propiedades de la Odds Ratio


.bold[una verdad parcial:] el resultado refleja un .bold[cambio en la distribución marginal] de educación de los hijos, no un cambio en la asociación de las variables. 
--
 Concretamente, se cuaduplicó la cantidad de gente que termina la universidad, independiente de su origen. 

.pull-top[

.pull-left[
.bold[Movilidad educacional 1980]
```{r, echo=FALSE}
pais_1980 <- matrix(c(160,20,20,20),2,2)
colnames(pais_1980) <- c("Hijos: NU","Hijos: U")
rownames(pais_1980) <- c("Padre: NU","Padres: U")
knitr::kable(pais_1980)
```
]
.pull-right[
.bold[Movilidad educacional 2020]
```{r, echo=FALSE}
pais_2020 <- matrix(c(160,20,80,80),2,2)
colnames(pais_2020) <- c("Hijos: NU","Hijos: U")
rownames(pais_2020) <-  c("Padre: NU","Padres: U")
knitr::kable(pais_2020)
```
]


<br>
--

.pull-bottom[

.full-width[La odds ratio es "inmune a cambios" en la distribución marginal de las variables, capturando sólo la asociación neta entre ellas ("margin-free association")] 

.pull-left[
$\hat{\theta}_{1980} =  \frac{160 \times 20}{20 \times 20} = 8$
]
.pull-right[
$\hat{\theta}_{2020} =  \frac{160 \times 80}{20 \times 80} = \frac{160 \times (4 \times 20)}{20 \times (4 \times 20)} = 8$
]
]
 
---
### Log Odds Ratio

Como sabemos, $\theta \in [0,\infty+)$. Esto crea un problema tanto para la .bold[interpretación] como para la .bold[inferencia estadística]. Por ejemplo:

- Supongamos que la odds ratio (hombres a mujeres) de tener un affair es $\theta = 20$.
- Por ende, la odds ratio (mujeres a hombres) de tener un affair es $\theta^{*} = 1/ \theta = 0.05$. 
- Ambos resultados indican el .bold[mismo nivel de asociación], pero uno parece mucho más grande que el otro.

--

Transformando $\theta$ a escala logarítmica permite mapear  $[0,\infty+) \to (-\infty,\infty+)$, creando una medida de asociación  simétrica. 

\begin{align}
  \theta &=  1/\theta^{*}  \quad \text{entonces} \\ \\
  \log(\theta) &= -1 \times \log(\theta^{*})
\end{align}

--
En nuestro ejemplo:
.pull-left[
```{r, warning=F, message=F}
log(20)
```
]

.pull-right[
```{r, warning=F, message=F}
log(0.05)
```
]

---
### Log Odds Ratio

<br>

-  $\log(\theta) \in (\infty-,\infty+)$ 

- $\theta=0$ indica igualdad de odds y, por lo tanto, independencia

- $\log(\theta) > 0$ indica que el éxito es más probable para el grupo en el numerador

- $\log(\theta) < 0$ indica que el éxito es más probable para el grupo en el denominador

- $\lvert \log(\theta) \rvert$ indica la fuerza de la asociación entre las variables

- Valores lejos de 0, en cualquier dirección, representan fuerte evidencia contra independencia



---
class: inverse, center, middle

## Inferencia para Medidas de Asociación

---
## Inferencia para medidas de Asociación

O, sobre como podemos saber si nuestros resultados no son, o no, producidos por el mero azar.

--

- Para responder esta pregunta debemos conocer la .bold[sampling distribution] de nuestro estimador, especialmente su _variabilidad_.

- Los parámetros que "generan" los datos no varían pero nuestra estimaciones si: de muestra en muestra.

--

<br>

.bold[Caso canónico] es el _promedio muestral_, para el cual sabemos que: $\bar{X} \sim \text{Normal}(\mu,\frac{\sigma}{\sqrt{n}})$
 - La desviación estándar del .bold[estimador] (en este caso, $\frac{\sigma}{\sqrt{n}}$ ) es lo que denominamos .bold[error estándar (SE)].

--

 - Por qué? Si $x_1, \dots, x_n$ son _iid_, entonces:
\begin{align}
\mathbb{Var}\big(\bar{X}\big) &= \mathbb{Var}\Big(\frac{x_{i} + ... + x_{n}}{n}\Big) = \frac{1}{n^2} \Big(\mathbb{Var}(x_{i}) + ... + \mathbb{Var}(x_{n})\Big) \\
 &= \frac{1}{n^2}( \sigma^2 + ... + \sigma^2 ) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n} 
\end{align}



---
### Inferencia para Diferencia de proporciones

Como recordarán de clases anteriores, asintóticamente, la "sampling distribution" de una proporción es:

$$\hat{p} \sim \text{Normal}(\mu,\sigma) \quad \quad \text{ donde }\mu = p \quad \text{ y }\quad \sigma^2 = p(1-p)/n$$

--

Por tanto, la "sampling distribution" de la diferencia entre dos proporciones _independientes_,  $\hat{\delta} = \hat{p}_{1} - \hat{p}_{2}$, es:


$$\text{Normal}\Big(\mu_{1} = p_{1}, \sigma_{1} = \sqrt{p_{1}(1-p_{1})/n_{1}}\Big) - \text{Normal}\Big(\mu_{2} = p_{2},  \sigma_{2} = \sqrt{p_{2}(1-p_{2})/n_{2}}\Big)$$
--

dado que para variables independiente X e Y: 
  - $\mathbb{E}(X - Y)  = E(X) -  E(Y)$ y $\mathbb{Var}(X - Y)  = \mathbb{Var}(X) + \mathbb{Var}(Y)$

--

  - $\text{Normal}() \pm \text{Normal}() \sim \text{Normal}()$,  entonces:

--

.content-box-secondary[
$$\hat{p}_{1} - \hat{p}_{2} \sim \text{Normal}\Big(\mu_{\delta} = p_{1} - p_{2}, \sigma_{\delta} = \sqrt{p_{1}(1-p_{1})/n_{1} + p_{2}(1-p_{2})/n_{2}}\Big)$$
]

---
### Inferencia para Diferencia de proporciones

.bold[Intervalo de confianza]
Podemos usar este resultado para construir un intervalo de confianza para $\hat{\delta} = \hat{p_{1}} - \hat{p_{2}}$, al (1 - $\alpha$)% de confianza. Para un nivel de significación estadística de $\alpha=0.05$,

\begin{align}
  95\% \text{ CI}_{\hat{\delta}} &= \hat{\delta} \pm 2 \times SE \\ \\
          &= (\hat{p_{1}} - \hat{p_{2}}) \pm 2  \sqrt{p_{1}(1-p_{1})/n_{1} + p_{2}(1-p_{2})/n_{2}}
\end{align}

<br>
.bold[Nota importante]: cuando no conocemos los _verdaderos_ parámetros reemplazamos por sus valores estimados  (en este caso, $\hat{p_{1}}$ y $\hat{p_{2}}$ en vez de $p_{1}$ y $p_{2}$).


---
### Inferencia para Diferencia de proporciones

$$95\% \text{ CI}_{\hat{\delta}} = (\hat{p_{1}} - \hat{p_{2}}) \pm 2  \sqrt{p_{1}(1-p_{1})/n_{1} + p_{2}(1-p_{2})/n_{2}}$$

En nuestro ejemplo,

.pull-left[
```{r, warning=F, message=F}
print(ctable)
```
]

.pull-right[
```{r, warning=F, message=F}
n1 = sum(ctable[2,])
n2 = sum(ctable[1,])
p1_hat = ctable[2,1]/n1
p2_hat = ctable[1,1]/n2

```
]


```{r diff-prop, eval=FALSE}
delta_hat = p1_hat - p2_hat
se = sqrt((p1_hat*(1 - p1_hat))/n1 +  (p2_hat*(1 - p2_hat))/n2)
ci95_delta= c(ll=(delta_hat - 2*se), ul=(delta_hat + 2*se)); print(ci95_delta)
```

--

.pull-left[
Nuestro 95% CI:
```{r diff-prop-out, ref.label="diff-prop", echo=FALSE}
```
]

.pull-right[
Versión automática con `prop.test()` en `R`:
```{r, echo=FALSE}
test_props <- prop.test(rev(ctable[,1]), rev(apply(ctable,1,sum)), p = NULL, alternative = "two.sided", correct=FALSE)
print(test_props$conf.int[c(1,2)])
```
]

```{css, echo=FALSE}
.pull-right ~ * { clear: unset; }
.pull-right + * { clear: both; }
```


---
### Inferencia para la Odds Ratio

- Cual es la .bold[sampling distribucion] de $\hat{\theta}$? 

--

  - Si para un proporción sabemos que $\hat{p} \sim \text{Normal}(\mu,\sigma) \quad \quad \text{ donde }\mu = p \quad \text{ y }\quad \sigma^2 = p(1-p)/n$
  

<br>
La sampling distribution de $\hat{\theta}$ debe ser ...

--


.pull-left[
$$\hat{\theta} = \frac{\hat{p}_{1}/(1 - \hat{p}_{1})}{\hat{p}_{2}/(1 - \hat{p}_{2})}  \sim \frac{\frac{\text{Normal}(\mu_{1},\sigma_{1})}{1 - \text{Normal}(\mu_{1},\sigma_{1})}}{\frac{\text{Normal}(\mu_{2},\sigma_{2})}{1 - \text{Normal}(\mu_{2},\sigma_{2})}}$$  
]

--
.pull-right[
![meme](meme.png)

Complicado ...
]



  
  
---
### Inferencia para la Odds Ratio

- Más conveniente hacer inferencia sobre $\log \hat{\theta}$

- Usando la definición de $\hat{\theta}$ como cross-product, obtenemos:


  $$\log \hat{\theta} = \log \frac{n_{11}n_{22}}{n_{12}n_{21}} = \log n_{11} + \log n_{22} - \log n_{12} - \log n_{21}$$

<br>
--

Importante resultado teórico: la sampling distribution de $\log \hat{\theta}$ es _asintóticamente_ normal:

$$\log(\hat{\theta}) \sim \text{Normal}(\mu,\sigma)$$
con parámetros _estimados_ por:

 - $\hat{\mu} = \log \hat{\theta}$ 
 
 - $\hat{\sigma} = \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}}$

---
### Inferencia para la Odds Ratio

.bold[Intervalo de confianza para log Odds ratio]

<br>
--

Podemos usar este resultado para construir un intervalo de confianza para el log Odds ratio, al (1 - $\alpha$) de confianza. Para un nivel de significación estadística de $\alpha=0.05$,

\begin{align}
  95\% \text{ CI}_{\log \hat{\theta}} &= \log \hat{\theta} \pm 2 \times SE \\ \\
          &= \log \hat{\theta} \pm 2 \times \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}} } 
\end{align}

<br>
--
.bold[Intervalo de confianza para Odds ratio]

Podemos obtener un intervalo de confianza estándar para la Odds ratio, al (1 - $\alpha$) de confianza tomando el exponencial del intervalo obtenido para $\log \hat{\theta}$.

\begin{align}
  95\% \text{ CI}_{\hat{\theta}} &= e^{\log \hat{\theta} \pm 2 \times \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}}} } 
\end{align}

---
### Intervalo de confianza para Odds ratio

\begin{align}
  95\% \text{ CI}_{\log \hat{\theta}} = \log \hat{\theta} \pm 2 \times \sqrt{ \frac{1}{n_{11}} + \frac{1}{n_{12}} + \frac{1}{n_{21}} + \frac{1}{n_{22}} } 
\end{align}

--

En nuestro ejemplo,

.pull-left[
```{r, warning=F, message=F}
print(ctable)
```
]

.pull-right[
```{r, warning=F, message=F}
n11 = ctable[1,1]
n12 = ctable[1,2]
n21 = ctable[2,1]
n22 = ctable[2,2]
log_theta_hat = log((n11 * n22) / (n12 * n21))
SE = sqrt(1/n11 + 1/n12 + 1/n21 + 1/n22)
```
]

<br>
.bold[Intervalos de confianza:]

```{r, warning=F, message=F}
ci95_log_theta = c(ll = (log_theta_hat - 2 * SE), ul = (log_theta_hat + 2 * SE))
print(ci95_log_theta)
```


---
class: inverse, center, middle


##Hasta la próxima clase. Gracias!


<br>
Mauricio Bucca <br>
https://mebucca.github.io/ <br>
github.com/mebucca




