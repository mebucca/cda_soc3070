---
title: "Análisis de Datos Categóricos"
subtitle: "Momentos & MLE"
author: "<br> Mauricio Bucca <br> [github.com/mebucca](https://github.com/mebucca) <br> mebucca@uc.cl"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["gentle-r.css","xaringan-themer.css"]
    df_print: default
    nature:
      ratio: 16:9
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      slideNumberFormat: "%current%"
editor_options: 
  
  chunk_output_type: console
---  

class: inverse, center, middle
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(tidyverse)
library(xaringanthemer)
style_duo_accent(primary_color = "#4953A6", secondary_color = "#FFD58D" ,
                 background_color = "#f8f7f3",
                 header_font_google = google_font("Archivo"),
                 text_font_google   = google_font("Inconsolata"), 
                 link_color="#DF484C"

)
```


#Valor Esperado de variables discretas

---
## Valor Esperado

El valor esperado de una variable es el análogo teórico de un promedio. Los posibles valores de la variable se ponderan por su probabilidad de ocurrencia. En el caso de variables discretas:

<br>
--

\begin{align}
\mathbb{E}(X) &= \sum_{i} x_{i} \times \mathbb{P}(X=x_{i}) \\
&\equiv  \sum_{i} x_{i} \times f(x_{i})
\end{align}

<br>
--
Es teórico porque esta información la podemos saber *a priori*, sin necesidad de datos. 

--

Análogamente, para variables continuas:

\begin{align}
\mathbb{E}(X) =  \int x f(x)dx
\end{align}

---
## Valor Esperado

Por ejemplo, supongamos que $Y$ es una variable que resulta de tirar un dado "justo".  ¿Cuál es el valor esperado de $Y$?

<br>
--

\begin{align}
\mathbb{E}(Y) &= \sum_{i}y_{i} \times \mathbb{P}(Y=y_{i})  \\ \\
     &=  1 \times  \frac{1}{6}+ 2 \times \frac{1}{6} + \dots + 6 \times \frac{1}{6}  \\ \\
     &= 3.5
\end{align}


---
## Valor Esperado, algunas propiedades útiles  

<br>
--

1) El valor esperado de una constante es una constante.

$$\mathbb{E}(c)=c$$
<br>
--

2) Si $X$ es una variable aleatoria y $c$ una constante, entonces 

$$\mathbb{E}(X + c)= \mathbb{E}(X) + c$$

<br>
--

3) Si $X$ es una variable aleatoria y $c$ una constante, entonces 

$$\mathbb{E}(c X)= c \mathbb{E}(X)$$
<br>
--

4) Si $X$ e $Y$ son variables aleatorias (sin importar si $X \bot Y$ o no), entonces

$$\mathbb{E}(X + Y)=  \mathbb{E}(X) + \mathbb{E}(Y)$$


---
## Valor Esperado, ejemplo

Por ejemplo, supongamos que $X_{i}$ es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 10 veces. El premio (G) es $ $1000$ de base, más el resultado de cada dado $i$ multiplicado por 100.
--

 ¿Cuánto es el premio esperado?

--

.center[
```{r, echo=FALSE, fig.height=6, fig.width=10,  message=FALSE, warning=FALSE, warning=FALSE}
library("tidyverse")

library("tidyverse")

n <- 10  # Number of dice rolls
simulations <- 10000  # Number of times to simulate the game

# Simulate rolling a die n times, for 'simulations' times
set.seed(123)  # For reproducibility
results <- replicate(simulations, sum(sample(1:6, n, replace = TRUE)))

# Calculate the prize for each simulation
prizes <- 1000 + 100 * results

# Create data frame for plotting
df <- tibble(prize = prizes)

# Calculate mean and standard deviation of the prizes
mean_prize <- mean(prizes)
std_dev <- sd(prizes)

# Bar plot of prizes
plot <- ggplot(df, aes(x = prize)) +
  geom_bar(aes(y = ..prop..), fill = "#FFD58D", color = "black", width = 100) +
  
  # Add a dashed line for the mean
  geom_vline(aes(xintercept = mean_prize), linetype="dashed", size = 1) +
  
  # Add a text label with the Greek letter µ for the mean
  annotate("text", x = mean_prize + 150, y = 0.075, label = expression(mu), size = 5) +
  labs(y = "P(Premio = x)", x = "Monto del Premio ($)", title = "Distribución del premio") +
  
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 22),
    axis.text.x = element_text(size = 22),
    axis.title.y = element_text(size = 24),
    axis.title.x = element_text(size = 24),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

print(plot)


```
]

---

## Valor Esperado, ejemplo

Por ejemplo, supongamos que $X_{i}$ es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 10 veces. El premio (G) es $ $1000$ de base, más el resultado de cada dado $i$ multiplicado por 100. ¿Cuánto es el premio esperado?

<br>
--

$$G = 1000 + \sum^{n=10}_{i=1} X_{i} \times 100 \quad \text{ por tanto,}$$

--

$$\mathbb{E}(G) = \mathbb{E}(1000 + \sum^{n=10}_{i=1} X_{i} \times 100)$$

--

$$\mathbb{E}(G) = 1000 + 100 \times \sum^{n=10}_{i=1}\mathbb{E}(X_{i})$$

--

$$\mathbb{E}(G) = 1000 + 10 (3.5 + 3.5 + \dots + 3.5)) = 1000 + 100 \times 10 \times 3.5 = \$4500$$ 

---
## Valor Esperado de variables discretas

###  Bernoulli

Si X es una variable Bernoulli, su valor esperado viene dado por:

<br>

\begin{align}
\mathbb{E}(X) = \sum_{i} x_{i} \times \mathbb{P}(X=x_{i}) &= \sum_{i} x_{i} \times p^{x_{i}}(1-p)^{1 - x_{i}} \\ 
     &= 1 \times p + 0 \times (1-p) \\ 
     &= p
\end{align}

--
### Binomial

Si X es una variable Binomial, su valor esperado viene dado por:

\begin{align}
\mathbb{E}(X) = np
\end{align}

--
.bold[Pregunta]: ¿Cuántas "Caras" debo esperar si tiro 200 monedas "justas"?

--

.bold[Respuesta]: $np = 200 \times 0.5 = 100$



---
class: inverse, center, middle


# Varianza de variables discretas

---
## Varianza 

La varianza de una variable aleatoria es el análogo teórico de la varianza de los datos.
--
 Mide cuánta dispersión existe en torno al centro (la media). Formalmente, en el caso de variables aleatorias discretas:

<br>

$$\mathbb{Var}(X) = \sum_{i} \bigg( x_{i} - \mathbb{E}(X) \bigg)^{2} \times f(x_{i})$$
<br>
--

Análogamente, para variables continuas:

\begin{align}
\mathbb{Var}(X) =  \int \bigg(x -  \mathbb{E}(X)\bigg)^{2} f(x)dx
\end{align}

<br>
--

Equivalentemente, 

$$\mathbb{Var}(X)  = \mathbb{E}\bigg( [ X -\mathbb{E}(X)]^{2}\bigg)$$
---
## Varianza, algunas propiedades útiles  

<br>
--

1) La varianza de una constante es cero.

$$\mathbb{Var}(c)=0$$

<br>
--

2) Si $X$ es una variable aleatoria y $c$ una constante, entonces 


$$\mathbb{Var}(X + c)= \mathbb{Var}(X)$$

<br>
--
3) Si $X$ es una variable aleatoria y $c$ una constante, entonces 
<br>

$$\mathbb{Var}(c X)= c^{2} \mathbb{Var}(X)$$

<br>
--

4) Si $X$ e $Y$ son dos variables aleatorias .bold[independientes], entonces


$$\mathbb{Var}(X \pm Y) =  \mathbb{Var}(X) + \mathbb{Var}(Y)$$



---
## Varianza, ejemplo  

Por ejemplo, supongamos que $X_{i}$ es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 10 veces. El premio (G) es $ $1000$ de base, más el resultado de cada dado $i$ multiplicado por 100.
--

 ¿Cuánto es la desviación estándar del premio?

--

.center[
```{r, echo=FALSE, fig.height=6, fig.width=10,  message=FALSE, warning=FALSE, warning=FALSE}
library("tidyverse")

library("tidyverse")

n <- 10  # Number of dice rolls
simulations <- 10000  # Number of times to simulate the game

# Simulate rolling a die n times, for 'simulations' times
set.seed(123)  # For reproducibility
results <- replicate(simulations, sum(sample(1:6, n, replace = TRUE)))

# Calculate the prize for each simulation
prizes <- 1000 + 100 * results

# Create data frame for plotting
df <- tibble(prize = prizes)

# Calculate mean and standard deviation of the prizes
mean_prize <- mean(prizes)
std_dev <- sd(prizes)

# Bar plot of prizes
plot <- ggplot(df, aes(x = prize)) +
  geom_bar(aes(y = ..prop..), fill = "#FFD58D", color = "black", width = 100) +
  
  # Add a dashed line for the mean
  geom_vline(aes(xintercept = mean_prize), linetype="dashed", size = 1) +
  
  # Add dashed lines for mean ± 1 standard deviation
  geom_vline(aes(xintercept = mean_prize - std_dev), linetype="dashed", color = "#4953A6", size = 1) +
  geom_vline(aes(xintercept = mean_prize + std_dev), linetype="dashed", color = "#4953A6", size = 1) +
  
  # Add a text label with the Greek letter µ for the mean
  annotate("text", x = mean_prize + 150, y = 0.075, label = expression(mu), size = 5) +
  
  # Add a text label with the Greek letter σ for the standard deviation
  annotate("text", x = mean_prize + std_dev + 150, y = 0.075, label = expression(sigma), size = 5, color = "#4953A6") +
  annotate("text", x = mean_prize - std_dev - 150, y = 0.075, label = expression(-sigma), size = 5, color = "#4953A6") +

  labs(y = "P(Premio = x)", x = "Monto del Premio ($)", title = "Distribución del premio") +
  
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 22),
    axis.text.x = element_text(size = 22),
    axis.title.y = element_text(size = 24),
    axis.title.x = element_text(size = 24),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

print(plot)


```
]


---
## Varianza, ejemplo  

Por ejemplo, supongamos que $X_{i}$ es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 10 veces. El premio (G) es $ $1000$ de base, más el resultado de cada dado $i$ multiplicado por 100. ¿Cuánto es la desviación estándar del premio?

<br>
--

$$G = 1000 + \sum^{n=10}_{i=1} X_{i} \times 100 \quad \text{ por tanto,}$$

--

$$\mathbb{Var}(G) = \mathbb{Var}(1000 + \sum^{n=10}_{i=1} X_{i} \times 100)$$

--

$$\mathbb{Var}(G) = \mathbb{Var}(1000) + 100^{2} \times \sum^{n=10}_{i=1}\mathbb{Var}(X_{i})$$

--

$$\mathbb{Var}(G) =  0 +  100^{2} \times 10 \times 2.9167 = \$291,670$$ 


<br>
--

$$\sigma_{G} = \sqrt{0 + 100 \times 100 \times 2.91670} = \$ 539.88$$

---
## Varianza de variables discretas

### Bernoulli

Si X es una variable Bernoulli, su varianza viene dada por:

<br>

\begin{align}
\mathbb{Var}(X) &= \sum_{i} \bigg( x_{i} - \mathbb{E}(X) \bigg)^{2} \times f(x_{i})  \\ \\
 &= (1 - \mathbb{E}(X))^{2} \times \mathbb{P}(X=1) + (0 - \mathbb{E}(X))^{2} \times \mathbb{P}(X=0) \\ \\
 &= (1 - p)^{2} \times p +  (0 - p)^{2} \times (1-p) \\ \\
 &=p^{2} − p^{3} + p − 2p^{2} + p^{3} \\ \\
 &=p(1-p)
\end{align}

---
## Varianza de variables discretas

### Binomial

Si X es una variable Binomial, su varianza viene dada por:

\begin{align}
\mathbb{Var}(X) = n \times p(1-p)
\end{align}

<br>
--
.bold[Pregunta]: ¿Cuánta variabilidad debo esperar en el número de "Caras" si tiro 200 monedas "justas"?

--

.bold[Respuesta]: varianza es $n \times p(1-p) = 200 \times 0.5 \times 0.5 = 50$. La desviación estándar es $\sqrt{50} = 7.01$.

---
## Varianza de variables Binomial

.bold[Ilustración via Monte Carlo simulation]

```{r}

# Repeat experiment of tossing 200 coins 10000 times
coins200 <- rbinom(10000, size=200, p=0.5)
glimpse(coins200)
moments = list(mean=mean(coins200), var=var(coins200))
print(moments)
```

---
## Varianza variable Bernoulli/Binomial

.center[

```{r, echo=FALSE, fig.height=8, fig.width=12,  message=FALSE, warning=FALSE}

library("tidyverse")

n = 100
p_values <- c(0.03, 0.5, 0.9)

# Creating a dataset with binomial distributions for each p
mydata <- tibble(
  x = rep(seq(from = 0, to = n, by = 1), length(p_values)),
  p = rep(p_values, each = n + 1),
  y = rep(NA, (n + 1) * length(p_values))
)

# Calculating probabilities for each p value
mydata$y <- with(mydata, dbinom(x, size = n, prob = p))

# Create a new column for facet titles
mydata$facet_title <- with(mydata, sprintf("Var(X) = 100 * %.2f * (1 - %.2f) = %.2f", p, p, 100 * p * (1 - p)))

# Create the plot
plot <- ggplot(data = mydata, mapping = aes(x = x, y = y)) +
  
  # Binomial Bars with black borders and no space between bars
  geom_bar(aes(fill = "#FFD58D"), color = "black", stat = "identity", position = "dodge", width = 1) +
  
  # Labels and title
  labs(y = "f(x)", x = "X = x", title = "Probability function of X | n=100, p") +
  
  # Facet based on the new title
  facet_wrap(~facet_title, scales = "free_y", ncol = 1) +
  
  # Styling using the theme you provided and removing legends
  scale_fill_identity(guide = "none") +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(), # This omits the y-axis labels
    axis.ticks.y = element_blank(), # This omits the y-axis ticks
    axis.text.x = element_text(size = 22),
    axis.title.y = element_text(size = 24),
    axis.title.x = element_text(size = 24),
    strip.text = element_text(size = 16, vjust = 0.5, hjust = 0.5),
    strip.background = element_blank(),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

print(plot)


```

]

---
## Varianza variable Binomial

.center[

```{r, echo=FALSE, fig.height=8, fig.width=12,  message=FALSE, warning=FALSE}
# Libraries
library(ggplot2)

# Define the binomial variance function
var_binom <- function(p, n) {
  n * p * (1 - p)
}

# Create the plot
plot <- ggplot(data = data.frame(p = 0), mapping = aes(x = p)) +
  stat_function(fun = var_binom, args = list(n = 1000), color = "black", size = 1.5) +
  xlim(0, 1) + 
  labs(title = "Varianza Binomial(n,p)", x = "p", y = expression(paste(n * p * (1 - p)))) +
  geom_point(aes(x = 0.5, y = 1000/4), fill = "#FFD58D", shape=21, size = 8) +
  annotate(geom = "text", x = 0.5, y = 1060/4, label = 'bold("n/4")', color = "black", parse = TRUE, size = 8) +
  
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 16),
    axis.title.y = element_text(size = 24),
    axis.title.x = element_text(size = 24),
    legend.position = "none",
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

plot

```

]

---
## Distribución Binomial es asintóticamente Normal

.pull-left[

<br>


- Si $X \sim \text{Binomial}(n,p)$, entonces:

$$ X \xrightarrow[]{d} \text{Normal}(\mu=np, \quad \sigma= \sqrt{np(1-p)})$$
(cuando $n \to \infty$ )


<br>

- Resultado muy conveniente


]

.pull-right[

```{r, echo=FALSE,  message=FALSE, warning=FALSE, warning=FALSE, fig.width=8}

library("tidyverse")

n = 60
p = 0.3

mydata <- tibble(x = seq(from = 0, to = n, by = 1),
                 norm = dnorm(x, mean = n * p, sd = sqrt(n * p * (1 - p))),
                 binom = dbinom(x, size = n, prob = p))

plot <- ggplot(data = mydata, mapping = aes(x = x)) +
  # Binomial Bars with black borders and no space between bars
  geom_bar(aes(y = binom, fill = "#FFD58D"), color = "black", stat = "identity", position = "dodge", width = 1) +
  # Normal Approximation Curve
  geom_line(aes(y = norm, color = "#4953A6"), size = 1.5) +
  
  labs(y = "f(x)", x = "x", title = "Probability function of X | n=60, p=0.3") +
  
  scale_color_manual(name = "", 
                     values = c("black"),
                     breaks = c("black"),
                     labels = c("N(mean=np, var=np(1-p))"),
                     guide = "legend") +
  
  scale_fill_manual(name = "", 
                    values = c("#FFD58D"),
                    breaks = c("#FFD58D"),
                    labels = c("Binomial(n=60,p=0.3)"),
                    guide = "legend") +
  
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 22),
    axis.text.x = element_text(size = 22),
    axis.title.y = element_text(size = 24),
    axis.title.x = element_text(size = 24),
    legend.text = element_text(size = 18),
    legend.position = "bottom",
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

print(plot)


```
]

---
class: inverse, center, middle

#Estimación
##Maximum Likelihood Estimation (MLE)

---
##Estimación 

.center[![estimado](estimado.jpeg)]

---
##Estimación 

.bold[Modelos estadísticos]:  ¿Cuáles son los valores más .bold[plausibles][1].footnote[[1] Notar que no dice "más probables"!] de los *parámetros* dado los *datos* que observamos? 


Ej. Supongamos que alguien lanza 100 veces la misma moneda y registra los resultados en una base de datos. Los datos se ven así:  

.pull-left[
```{r, echo=FALSE, fig.height=5, fig.width=6,  message=FALSE, warning=FALSE}
library("tidyverse")
set.seed(481)

data_coins <- data.frame(X = rbinom(n=100, size=1, prob=0.8))

data_coins %>% 
  ggplot(aes(x=factor(X))) + 
    geom_bar(aes(fill=factor(X)), width = 1, color = "black") + 
    geom_text(aes(label=..count.., y = ..count..), stat='count', vjust=-0.5, size = 5) + 
    scale_fill_manual(values = c("#FFD58D", "#FFD58D")) + 
    labs(y="Recuentos", x="", title="") +
    guides(fill=FALSE) + 
    ylim(0,90) +
    theme_minimal() +
    theme(
      axis.text.y = element_text(size = 22),
      axis.text.x = element_text(size = 22),
      axis.title.y = element_text(size = 24),
      axis.title.x = element_text(size = 24),
      panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
    )
```
]

--

.pull-right[

- Lo que vemos en la izquierda son .bold[datos]

- Datos: realización de $n$ variables aleatorias 

- Normalmente *no conocemos* la distribución de las variables

- Datos nos dan una pista sobre cuál podría ser esa distribución

- .bold[Estadística]: aprender de los datos para .bold[*estimar*] los parámetros que los generan

]

---
##Estimación via Maximum Likelihood (MLE) 

Previamente lanzamos la misma moneda 100 veces y obtuvimos "Cara" (1) 82 veces.
--
 ¿Qué valor de $p$ es más plausible ("likely") que genere estos datos?

MLE es justamente la formalización de esta pregunta. Pasos:

--

1) Decidir sobre la distribución subyacente que genera los datos. En este caso, podemos asumir que: 

  * Cada lanzamiento $X_{1}, X_{2}, \dots X_{100} \sim \text{Bernoulli}(p)$, donde X's son $iid$ 

--

2)  Escribir una función que cuantifique la plausibilidad de diferentes valores del parámetro. Dicha función se denomina .bold[likelihood function]: 

<br>
  * $\mathcal{L}(p \mid \text{ Datos}) = \mathbb{P}(\text{ Datos : \{1,0,1,1,....0,1\}} | \text{ } p)$

<br>
--

  * $\mathcal{L}(p \mid \text{ Datos}) = \mathbb{P}(x_{1})\mathbb{P}(x_{2}) \dots \mathbb{P}(x_{100}) = p^{82}(1-p)^{18}$


---
##Estimación via Maximum Likelihood (MLE) 

Podemos inspeccionar visualmente la "likelihood" de diferentes valores $p$.

<br>

.center[
```{r, echo=FALSE, fig.height=5, fig.width=9,  message=FALSE, warning=FALSE}
library("tidyverse")

plot <- ggplot(data = data.frame(p = 0), mapping = aes(x = p))

binom_distrib <- function(p,n,k) (p^(k))*((1-p)^(n-k))

plot + 
  stat_function(fun = binom_distrib, args = list(n= 100, k= 82), size=1.5, color = "#FFD58D") + 
  xlim(0,1) + 
  labs(title="Likelihood of p", x="p", y=expression(paste(p^{82}, (1-p)^{18}))) +
  guides(fill=FALSE, color=FALSE) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 22),
    axis.text.x = element_text(size = 22),
    axis.title.y = element_text(size = 24),
    axis.title.x = element_text(size = 24),
    panel.border = element_rect(color = "black", fill = NA, linewidth = 1)
  )

```
]

Intuitivamente: habiendo obtenido 82 caras, $p=0.82$ es el valor más plausible de $p$


---

##Estimación via Maximum Likelihood (MLE) 

3) Encontrar matemáticamente el valor de $p$ que maximiza $\mathcal{L}(p \mid \text{ Datos})$.


- $\mathcal{L}(p \mid \text{ Datos}) = \mathbb{P}(x_{1})\mathbb{P}(x_{2}) \dots \mathbb{P}(x_{n}) =\prod_{i=1}^{n} f(x_{i}) =  p^{k}(1-p)^{n-k} \quad \text{   donde  } k= \sum x_{i}$

--

- Para facilitar el cálculo tomamos logaritmo natural en ambos lados (.bold[log-likelihood])

  - $\ell\ell(p) = \ln \mathcal{L}(p \mid \text{ Datos})  = k \ln(p) + (n - k) \ln(1-p)$ 

--
-  Calcular la primera* derivada de $\ell\ell(p)$ con respecto a $p$: pendiente de la curva en cada valor de $p$.

  - $\ell\ell^{\text{ '}}(p) = \frac{k}{p} -  \frac{n-k}{1-p}$

--

- Encontrar el máximo de la función $\ell\ell(p)$: valor de $p$ en el cual la curva no cambia, es decir cuando $\ell\ell^{\text{ '}}(p)=0$ 

  - $\frac{k}{p} -  \frac{n-k}{1-p} = 0$
  
--

- Después de resolver por $p$ obtenemos:
  
   $$p = \frac{k}{n} = \frac{\sum x_{i}}{n}$$


---
##Estimación via Maximum Likelihood (MLE) 

<br>

- El estimador ML de $p$ es ....


- $\hat{p} = \frac{\sum x_{i}}{n}$


- Es decir, el porcentaje de 1's en la muestra!

--

- Intuitivo y elegante


---
##Estimación via Maximum Likelihood (MLE) 

.bold[Generalización]

<br>


$$\hat{\boldsymbol{\beta}}_{MLE} = \underset{\beta}{\arg\max\ } \mathcal{L}(\boldsymbol{\beta} \mid \boldsymbol{X})$$
$\hat{\boldsymbol{\beta}}$ es el MLE de $\boldsymbol{\beta}$ si es el(los) valor(es) que maximiza(n) la "likelihood function", condicional en los datos observados.

<br>
--

- Recordar que   $\mathcal{L}(\boldsymbol{\beta} \mid \boldsymbol{X}) = \mathbb{P}(\boldsymbol{X} \mid \boldsymbol{\beta})$.
  
--

- Requiere especificar de antemano la distribución conjunta de las observaciones (dif. de OLS, por ejemplo).

--

- ML es probablemente el approach de estimación más popular. 

--

- Intuitivo, pero, por lo general, no tan simple como el ejemplo que vimos hoy.

--

- Normalmente la maximización se realiza numéricamente (ej. método Newton–Raphson)

---
class: fullscreen,left, top, text-white
background-image: url(valdorcia.jpeg)

##Estimación via Maximum Likelihood (MLE) 

---

class: inverse, center, middle

.huge[
**Hasta la próxima clase. Gracias!**
]

<br>
Mauricio Bucca <br>
https://mebucca.github.io/ <br>
github.com/mebucca




