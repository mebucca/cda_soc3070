<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Análisis de Datos Categóricos</title>
    <meta charset="utf-8" />
    <meta name="author" content="  Mauricio Bucca   github.com/mebucca   mebucca@uc.cl" />
    <script src="libs/header-attrs-2.29/header-attrs.js"></script>
    <link rel="stylesheet" href="gentle-r.css" type="text/css" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Análisis de Datos Categóricos
]
.subtitle[
## Momentos &amp; MLE
]
.author[
### <br> Mauricio Bucca <br> <a href="https://github.com/mebucca">github.com/mebucca</a> <br> <a href="mailto:mebucca@uc.cl" class="email">mebucca@uc.cl</a>
]

---


class: inverse, center, middle



#Valor Esperado de variables discretas

---
## Valor Esperado

El valor esperado de una variable es el análogo teórico de un promedio. Los posibles valores de la variable se ponderan por su probabilidad de ocurrencia. En el caso de variables discretas:

&lt;br&gt;
--

`\begin{align}
\mathbb{E}(X) &amp;= \sum_{i} x_{i} \times \mathbb{P}(X=x_{i}) \\
&amp;\equiv  \sum_{i} x_{i} \times f(x_{i})
\end{align}`

&lt;br&gt;
--
Es teórico porque esta información la podemos saber *a priori*, sin necesidad de datos. 

--

Análogamente, para variables continuas:

`\begin{align}
\mathbb{E}(X) =  \int x f(x)dx
\end{align}`

---
## Valor Esperado

Por ejemplo, supongamos que `\(Y\)` es una variable que resulta de tirar un dado "justo".  ¿Cuál es el valor esperado de `\(Y\)`?

&lt;br&gt;
--

`\begin{align}
\mathbb{E}(Y) &amp;= \sum_{i}y_{i} \times \mathbb{P}(Y=y_{i})  \\ \\
     &amp;=  1 \times  \frac{1}{6}+ 2 \times \frac{1}{6} + \dots + 6 \times \frac{1}{6}  \\ \\
     &amp;= 3.5
\end{align}`


---
## Valor Esperado, algunas propiedades útiles  

&lt;br&gt;
--

1) El valor esperado de una constante es una constante.

`$$\mathbb{E}(c)=c$$`
&lt;br&gt;
--

2) Si `\(X\)` es una variable aleatoria y `\(c\)` una constante, entonces 

`$$\mathbb{E}(X + c)= \mathbb{E}(X) + c$$`

&lt;br&gt;
--

3) Si `\(X\)` es una variable aleatoria y `\(c\)` una constante, entonces 

`$$\mathbb{E}(c X)= c \mathbb{E}(X)$$`
&lt;br&gt;
--

4) Si `\(X\)` e `\(Y\)` son variables aleatorias (sin importar si `\(X \bot Y\)` o no), entonces

`$$\mathbb{E}(X + Y)=  \mathbb{E}(X) + \mathbb{E}(Y)$$`


---
## Valor Esperado, ejemplo

Por ejemplo, supongamos que `\(X_{i}\)` es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 10 veces. El premio (G) es $ `\(1000\)` de base, más el resultado de cada dado `\(i\)` multiplicado por 100.
--

 ¿Cuánto es el premio esperado?

--

.center[
![](class_4_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;
]

---

## Valor Esperado, ejemplo

Por ejemplo, supongamos que `\(X_{i}\)` es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 10 veces. El premio (G) es $ `\(1000\)` de base, más el resultado de cada dado `\(i\)` multiplicado por 100. ¿Cuánto es el premio esperado?

&lt;br&gt;
--

`$$G = 1000 + \sum^{n=10}_{i=1} X_{i} \times 100 \quad \text{ por tanto,}$$`

--

`$$\mathbb{E}(G) = \mathbb{E}(1000 + \sum^{n=10}_{i=1} X_{i} \times 100)$$`

--

`$$\mathbb{E}(G) = 1000 + 100 \times \sum^{n=10}_{i=1}\mathbb{E}(X_{i})$$`

--

$$\mathbb{E}(G) = 1000 + 10 (3.5 + 3.5 + \dots + 3.5)) = 1000 + 100 \times 10 \times 3.5 = \$4500$$ 

---
## Valor Esperado de variables discretas

###  Bernoulli

Si X es una variable Bernoulli, su valor esperado viene dado por:

&lt;br&gt;

`\begin{align}
\mathbb{E}(X) = \sum_{i} x_{i} \times \mathbb{P}(X=x_{i}) &amp;= \sum_{i} x_{i} \times p^{x_{i}}(1-p)^{1 - x_{i}} \\ 
     &amp;= 1 \times p + 0 \times (1-p) \\ 
     &amp;= p
\end{align}`

--
### Binomial

Si X es una variable Binomial, su valor esperado viene dado por:

`\begin{align}
\mathbb{E}(X) = np
\end{align}`

--
.bold[Pregunta]: ¿Cuántas "Caras" debo esperar si tiro 200 monedas "justas"?

--

.bold[Respuesta]: `\(np = 200 \times 0.5 = 100\)`



---
class: inverse, center, middle


# Varianza de variables discretas

---
## Varianza 

La varianza de una variable aleatoria es el análogo teórico de la varianza de los datos.
--
 Mide cuánta dispersión existe en torno al centro (la media). Formalmente, en el caso de variables aleatorias discretas:

&lt;br&gt;

`$$\mathbb{Var}(X) = \sum_{i} \bigg( x_{i} - \mathbb{E}(X) \bigg)^{2} \times f(x_{i})$$`
&lt;br&gt;
--

Análogamente, para variables continuas:

`\begin{align}
\mathbb{Var}(X) =  \int \bigg(x -  \mathbb{E}(X)\bigg)^{2} f(x)dx
\end{align}`

&lt;br&gt;
--

Equivalentemente, 

`$$\mathbb{Var}(X)  = \mathbb{E}\bigg( [ X -\mathbb{E}(X)]^{2}\bigg)$$`
---
## Varianza, algunas propiedades útiles  

&lt;br&gt;
--

1) La varianza de una constante es cero.

`$$\mathbb{Var}(c)=0$$`

&lt;br&gt;
--

2) Si `\(X\)` es una variable aleatoria y `\(c\)` una constante, entonces 


`$$\mathbb{Var}(X + c)= \mathbb{Var}(X)$$`

&lt;br&gt;
--
3) Si `\(X\)` es una variable aleatoria y `\(c\)` una constante, entonces 
&lt;br&gt;

`$$\mathbb{Var}(c X)= c^{2} \mathbb{Var}(X)$$`

&lt;br&gt;
--

4) Si `\(X\)` e `\(Y\)` son dos variables aleatorias .bold[independientes], entonces


`$$\mathbb{Var}(X \pm Y) =  \mathbb{Var}(X) + \mathbb{Var}(Y)$$`



---
## Varianza, ejemplo  

Por ejemplo, supongamos que `\(X_{i}\)` es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 10 veces. El premio (G) es $ `\(1000\)` de base, más el resultado de cada dado `\(i\)` multiplicado por 100.
--

 ¿Cuánto es la desviación estándar del premio?

--

.center[
![](class_4_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]


---
## Varianza, ejemplo  

Por ejemplo, supongamos que `\(X_{i}\)` es la variable que resulta de tirar un dado "justo". Participamos de un concurso que consiste en tirar el mismo dado 10 veces. El premio (G) es $ `\(1000\)` de base, más el resultado de cada dado `\(i\)` multiplicado por 100. ¿Cuánto es la desviación estándar del premio?

&lt;br&gt;
--

`$$G = 1000 + \sum^{n=10}_{i=1} X_{i} \times 100 \quad \text{ por tanto,}$$`

--

`$$\mathbb{Var}(G) = \mathbb{Var}(1000 + \sum^{n=10}_{i=1} X_{i} \times 100)$$`

--

`$$\mathbb{Var}(G) = \mathbb{Var}(1000) + 100^{2} \times \sum^{n=10}_{i=1}\mathbb{Var}(X_{i})$$`

--

$$\mathbb{Var}(G) =  0 +  100^{2} \times 10 \times 2.9167 = \$291,670$$ 


&lt;br&gt;
--

$$\sigma_{G} = \sqrt{0 + 100 \times 100 \times 2.91670} = \$ 539.88$$

---
## Varianza de variables discretas

### Bernoulli

Si X es una variable Bernoulli, su varianza viene dada por:

&lt;br&gt;

`\begin{align}
\mathbb{Var}(X) &amp;= \sum_{i} \bigg( x_{i} - \mathbb{E}(X) \bigg)^{2} \times f(x_{i})  \\ \\
 &amp;= (1 - \mathbb{E}(X))^{2} \times \mathbb{P}(X=1) + (0 - \mathbb{E}(X))^{2} \times \mathbb{P}(X=0) \\ \\
 &amp;= (1 - p)^{2} \times p +  (0 - p)^{2} \times (1-p) \\ \\
 &amp;=p^{2} − p^{3} + p − 2p^{2} + p^{3} \\ \\
 &amp;=p(1-p)
\end{align}`

---
## Varianza de variables discretas

### Binomial

Si X es una variable Binomial, su varianza viene dada por:

`\begin{align}
\mathbb{Var}(X) = n \times p(1-p)
\end{align}`

&lt;br&gt;
--
.bold[Pregunta]: ¿Cuánta variabilidad debo esperar en el número de "Caras" si tiro 200 monedas "justas"?

--

.bold[Respuesta]: varianza es `\(n \times p(1-p) = 200 \times 0.5 \times 0.5 = 50\)`. La desviación estándar es `\(\sqrt{50} = 7.01\)`.

---
## Varianza de variables Binomial

.bold[Ilustración via Monte Carlo simulation]


``` r
# Repeat experiment of tossing 200 coins 10000 times
coins200 &lt;- rbinom(10000, size=200, p=0.5)
glimpse(coins200)
```

```
##  int [1:10000] 103 86 92 112 99 105 94 112 95 95 ...
```

``` r
moments = list(mean=mean(coins200), var=var(coins200))
print(moments)
```

```
## $mean
## [1] 99.997
## 
## $var
## [1] 49.68256
```

---
## Varianza variable Bernoulli/Binomial

.center[

![](class_4_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;

]

---
## Varianza variable Binomial

.center[

![](class_4_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;

]

---
## Distribución Binomial es asintóticamente Normal

.pull-left[

&lt;br&gt;


- Si `\(X \sim \text{Binomial}(n,p)\)`, entonces:

$$ X \xrightarrow[]{d} \text{Normal}(\mu=np, \quad \sigma= \sqrt{np(1-p)})$$
(cuando `\(n \to \infty\)` )


&lt;br&gt;

- Resultado muy conveniente


]

.pull-right[

![](class_4_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]

---
class: inverse, center, middle

#Estimación
##Maximum Likelihood Estimation (MLE)

---
##Estimación 

.center[![estimado](estimado.jpeg)]

---
##Estimación 

.bold[Modelos estadísticos]:  ¿Cuáles son los valores más .bold[plausibles][1].footnote[[1] Notar que no dice "más probables"!] de los *parámetros* dado los *datos* que observamos? 


Ej. Supongamos que alguien lanza 100 veces la misma moneda y registra los resultados en una base de datos. Los datos se ven así:  

.pull-left[
![](class_4_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]

--

.pull-right[

- Lo que vemos en la izquierda son .bold[datos]

- Datos: realización de `\(n\)` variables aleatorias 

- Normalmente *no conocemos* la distribución de las variables

- Datos nos dan una pista sobre cuál podría ser esa distribución

- .bold[Estadística]: aprender de los datos para .bold[*estimar*] los parámetros que los generan

]

---
##Estimación via Maximum Likelihood (MLE) 

Previamente lanzamos la misma moneda 100 veces y obtuvimos "Cara" (1) 82 veces.
--
 ¿Qué valor de `\(p\)` es más plausible ("likely") que genere estos datos?

MLE es justamente la formalización de esta pregunta. Pasos:

--

1) Decidir sobre la distribución subyacente que genera los datos. En este caso, podemos asumir que: 

  * Cada lanzamiento `\(X_{1}, X_{2}, \dots X_{100} \sim \text{Bernoulli}(p)\)`, donde X's son `\(iid\)` 

--

2)  Escribir una función que cuantifique la plausibilidad de diferentes valores del parámetro. Dicha función se denomina .bold[likelihood function]: 

&lt;br&gt;
  * `\(\mathcal{L}(p \mid \text{ Datos}) = \mathbb{P}(\text{ Datos : \{1,0,1,1,....0,1\}} | \text{ } p)\)`

&lt;br&gt;
--

  * `\(\mathcal{L}(p \mid \text{ Datos}) = \mathbb{P}(x_{1})\mathbb{P}(x_{2}) \dots \mathbb{P}(x_{100}) = p^{82}(1-p)^{18}\)`


---
##Estimación via Maximum Likelihood (MLE) 

Podemos inspeccionar visualmente la "likelihood" de diferentes valores `\(p\)`.

&lt;br&gt;

.center[
![](class_4_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

Intuitivamente: habiendo obtenido 82 caras, `\(p=0.82\)` es el valor más plausible de `\(p\)`


---

##Estimación via Maximum Likelihood (MLE) 

3) Encontrar matemáticamente el valor de `\(p\)` que maximiza `\(\mathcal{L}(p \mid \text{ Datos})\)`.


- `\(\mathcal{L}(p \mid \text{ Datos}) = \mathbb{P}(x_{1})\mathbb{P}(x_{2}) \dots \mathbb{P}(x_{n}) =\prod_{i=1}^{n} f(x_{i}) =  p^{k}(1-p)^{n-k} \quad \text{   donde  } k= \sum x_{i}\)`

--

- Para facilitar el cálculo tomamos logaritmo natural en ambos lados (.bold[log-likelihood])

  - `\(\ell\ell(p) = \ln \mathcal{L}(p \mid \text{ Datos})  = k \ln(p) + (n - k) \ln(1-p)\)` 

--
-  Calcular la primera* derivada de `\(\ell\ell(p)\)` con respecto a `\(p\)`: pendiente de la curva en cada valor de `\(p\)`.

  - `\(\ell\ell^{\text{ '}}(p) = \frac{k}{p} -  \frac{n-k}{1-p}\)`

--

- Encontrar el máximo de la función `\(\ell\ell(p)\)`: valor de `\(p\)` en el cual la curva no cambia, es decir cuando `\(\ell\ell^{\text{ '}}(p)=0\)` 

  - `\(\frac{k}{p} -  \frac{n-k}{1-p} = 0\)`
  
--

- Después de resolver por `\(p\)` obtenemos:
  
   `$$p = \frac{k}{n} = \frac{\sum x_{i}}{n}$$`


---
##Estimación via Maximum Likelihood (MLE) 

&lt;br&gt;

- El estimador ML de `\(p\)` es ....


- `\(\hat{p} = \frac{\sum x_{i}}{n}\)`


- Es decir, el porcentaje de 1's en la muestra!

--

- Intuitivo y elegante


---
##Estimación via Maximum Likelihood (MLE) 

.bold[Generalización]

&lt;br&gt;


`$$\hat{\boldsymbol{\beta}}_{MLE} = \underset{\beta}{\arg\max\ } \mathcal{L}(\boldsymbol{\beta} \mid \boldsymbol{X})$$`
`\(\hat{\boldsymbol{\beta}}\)` es el MLE de `\(\boldsymbol{\beta}\)` si es el(los) valor(es) que maximiza(n) la "likelihood function", condicional en los datos observados.

&lt;br&gt;
--

- Recordar que   `\(\mathcal{L}(\boldsymbol{\beta} \mid \boldsymbol{X}) = \mathbb{P}(\boldsymbol{X} \mid \boldsymbol{\beta})\)`.
  
--

- Requiere especificar de antemano la distribución conjunta de las observaciones (dif. de OLS, por ejemplo).

--

- ML es probablemente el approach de estimación más popular. 

--

- Intuitivo, pero, por lo general, no tan simple como el ejemplo que vimos hoy.

--

- Normalmente la maximización se realiza numéricamente (ej. método Newton–Raphson)

---
class: fullscreen,left, top, text-white
background-image: url(valdorcia.jpeg)

##Estimación via Maximum Likelihood (MLE) 

---

class: inverse, center, middle

.huge[
**Hasta la próxima clase. Gracias!**
]

&lt;br&gt;
Mauricio Bucca &lt;br&gt;
https://mebucca.github.io/ &lt;br&gt;
github.com/mebucca




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
  "ratio": "16:9",
  "highlightStyle": "github",
  "highlightLines": true,
  "countIncrementalSlides": true,
  "slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
